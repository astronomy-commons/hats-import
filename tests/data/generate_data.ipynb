{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test data\n",
    "\n",
    "This directory contains very small, toy, data sets that are used\n",
    "for unit tests.\n",
    "\n",
    "## Object catalog: small_sky\n",
    "\n",
    "This \"object catalog\" is 131 randomly generated radec values.\n",
    "\n",
    "- All radec positions are in the Healpix pixel order 0, pixel 11.\n",
    "- IDs are integers from 700-831.\n",
    "\n",
    "The following are imports and paths that are used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import lsdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow.parquet as pq\n",
    "from dask.distributed import Client\n",
    "from hats.io.file_io import remove_directory\n",
    "\n",
    "from hats_import import pipeline_with_client, ImportArguments\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "\n",
    "hats_import_dir = \".\"\n",
    "client = Client(n_workers=1, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky\n",
    "\n",
    "This \"object catalog\" is 131 randomly generated radec values.\n",
    "\n",
    "- All radec positions are in the Healpix pixel order 0, pixel 11.\n",
    "- IDs are integers from 700-831.\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_object_catalog\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        highest_healpix_order=5,\n",
    "        output_artifact_name=\"small_sky_object_catalog\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "        addl_hats_properties={\"hats_cols_default\": [\"ra\", \"dec\", \"id\"]},\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source catalog: small_sky_source\n",
    "\n",
    "This \"source catalog\" is 131 detections at each of the 131 objects\n",
    "in the \"small_sky\" catalog. These have a random magnitude, MJD, and\n",
    "band (selected from ugrizy). The full script that generated the values\n",
    "can be found [here](https://github.com/delucchi-cmu/hipscripts/blob/main/twiddling/small_sky_source.py)\n",
    "\n",
    "The catalog was generated with the following snippet, using raw data\n",
    "from the `hats-import` file.\n",
    "\n",
    "NB: `pixel_threshold=3000` is set just to make sure that we're generating\n",
    "a handful of files at various healpix orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_source_catalog\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky_source\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        ra_column=\"source_ra\",\n",
    "        dec_column=\"source_dec\",\n",
    "        catalog_type=\"source\",\n",
    "        highest_healpix_order=5,\n",
    "        pixel_threshold=3000,\n",
    "        drop_empty_siblings=False,\n",
    "        output_artifact_name=\"small_sky_source_catalog\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "tmp_path.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested catalog: small_sky_nested_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sky_object = lsdb.read_hats(\"small_sky_object_catalog\", columns=\"all\")\n",
    "small_sky_source = lsdb.read_hats(\"small_sky_source_catalog\")\n",
    "small_sky_nested = small_sky_object.join_nested(\n",
    "    small_sky_source, left_on=\"id\", right_on=\"object_id\", nested_column_name=\"lc\"\n",
    ")\n",
    "lsdb.io.to_hats(\n",
    "    small_sky_nested,\n",
    "    base_catalog_path=\"small_sky_nested_catalog\",\n",
    "    catalog_name=\"small_sky_nested_catalog\",\n",
    "    histogram_order=5,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malformed Catalogs: bad_schemas and wrong_files_and_rows\n",
    "\n",
    "These datasets are designed to fail verification tests.\n",
    "They are generated by mangling `small_sky_object_catalog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input data that will be used to generate the malformed catalogs.\n",
    "input_dataset_path = Path(hats_import_dir) / \"small_sky_object_catalog\" / \"dataset\"\n",
    "input_ds = pds.parquet_dataset(input_dataset_path / \"_metadata\")\n",
    "\n",
    "# Unit tests expect the Npix=11 data file\n",
    "input_frag = next(frag for frag in input_ds.get_fragments() if frag.path.endswith(\"Npix=11.parquet\"))\n",
    "frag_key = Path(input_frag.path).relative_to(input_dataset_path)\n",
    "input_tbl = input_frag.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_write_metadata(output_dataset_path: Path, schema: pa.Schema | None = None) -> None:\n",
    "    schema = schema or input_tbl.schema\n",
    "    dataset = pds.dataset(output_dataset_path)\n",
    "    metadata_collector = []\n",
    "    for frag in dataset.get_fragments():\n",
    "        frag.ensure_complete_metadata()\n",
    "        frag.metadata.set_file_path(str(Path(frag.path).relative_to(output_dataset_path)))\n",
    "        metadata_collector.append(frag.metadata)\n",
    "    pq.write_metadata(\n",
    "        schema=schema, where=output_dataset_path / \"_metadata\", metadata_collector=metadata_collector\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bad_schemas\n",
    "\n",
    "This dataset is designed to fail all schema verification tests.\n",
    "\n",
    "```\n",
    "bad_schemas/\n",
    "|- dataset/\n",
    "    |- _common_metadata.import_truth        # mimics schema provided by user upon import\n",
    "    |- _common_metadata                     # wrong types\n",
    "    |- _metadata                            # wrong file-level metadata\n",
    "    |- Norder=0/Dir=0/\n",
    "        |- Npix=11.parquet                  # direct copy of input\n",
    "        |- Npix=11.extra_column.parquet     # extra column\n",
    "        |- Npix=11.missing_column.parquet   # missing column\n",
    "        |- Npix=11.wrong_dtypes.parquet     # wrong types\n",
    "        |- Npix=11.wrong_metadata.parquet   # wrong metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset_path = Path(\".\") / \"bad_schemas\" / \"dataset\"\n",
    "remove_directory(output_dataset_path)\n",
    "\n",
    "# Existing files may result in unexpected metadata output.\n",
    "if output_dataset_path.parent.exists() and any(output_dataset_path.parent.iterdir()):\n",
    "    raise FileExistsError(\"bad_schemas directory exists and is not empty. Remove it and try again.\")\n",
    "\n",
    "# We will create the following files using input_frag\n",
    "ffrag_out = output_dataset_path / frag_key\n",
    "fextra_col = ffrag_out.with_suffix(\".extra_column.parquet\")\n",
    "fmissing_col = ffrag_out.with_suffix(\".missing_column.parquet\")\n",
    "fwrong_types = ffrag_out.with_suffix(\".wrong_dtypes.parquet\")\n",
    "fwrong_metadata = ffrag_out.with_suffix(\".wrong_metadata.parquet\")\n",
    "\n",
    "ffrag_out.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a direct copy of input_frag for all files that will be recorded in the _metadata file\n",
    "for file_out in [ffrag_out, fmissing_col, fextra_col, fwrong_types]:\n",
    "    shutil.copy(input_frag.path, file_out)\n",
    "\n",
    "# Write a _metadata that has the correct schema except for file-level metadata\n",
    "metadata = input_tbl.schema.metadata or {}\n",
    "metadata.update({b\"extra key\": b\"extra value\"})\n",
    "collect_and_write_metadata(output_dataset_path, schema=input_tbl.schema.with_metadata(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new data files using incorrect schemas.\n",
    "\n",
    "# Drop a column\n",
    "pq.write_table(input_tbl.drop_columns(\"dec_error\"), fmissing_col)\n",
    "\n",
    "# Add an extra column\n",
    "extra_col = pa.array(range(len(input_tbl)))\n",
    "extra_col_tbl = input_tbl.add_column(5, pa.field(\"extra\", pa.int64()), extra_col)\n",
    "pq.write_table(extra_col_tbl, fextra_col)\n",
    "\n",
    "# Mangle file-level metadata\n",
    "wrong_metadata = {\"bad key\": \"bad value\"}\n",
    "pq.write_table(input_tbl.replace_schema_metadata(wrong_metadata), fwrong_metadata)\n",
    "\n",
    "# Change some types\n",
    "wrong_dtypes_fields = [\n",
    "    fld if not fld.name.startswith(\"ra\") else fld.with_type(pa.float16()) for fld in input_tbl.schema\n",
    "]\n",
    "wrong_dtypes_schema = pa.schema(wrong_dtypes_fields, metadata=input_tbl.schema.metadata)\n",
    "pq.write_table(input_tbl.cast(wrong_dtypes_schema), fwrong_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a _common_metadata with the wrong dtypes.\n",
    "pq.write_metadata(schema=wrong_dtypes_schema, where=output_dataset_path / \"_common_metadata\")\n",
    "\n",
    "# Write a _common_metadata with the correct schema but no hats columns.\n",
    "# This mimics a schema that could have been passed as 'use_schema_file' upon import.\n",
    "fimport_schema = (output_dataset_path / \"_common_metadata\").with_suffix(\".import_truth\")\n",
    "hats_cols = [\"_healpix_29\", \"Norder\", \"Dir\", \"Npix\"]\n",
    "import_schema = pa.schema([fld for fld in input_tbl.schema if fld.name not in hats_cols])\n",
    "pq.write_metadata(schema=import_schema, where=fimport_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrong_files_and_rows\n",
    "\n",
    "This dataset is designed to fail the following verification tests:\n",
    "\n",
    "- Files listed in metadata match files on disk.\n",
    "- Row counts in metadata match row counts on disk and (if provided) user-supplied truth.\n",
    "- `hats.io.validation.is_valid_catalog`\n",
    "\n",
    "```\n",
    "wrong_files_and_rows/\n",
    "|- properties                               # direct copy of input\n",
    "|- dataset/\n",
    "    |- _common_metadata                     # direct copy of input\n",
    "    |- _metadata                            # missing file 'Npix=11.extra_file.parquet'\n",
    "    |- Norder=0/Dir=0/\n",
    "        |- Npix=11.parquet                  # direct copy of input\n",
    "        |- Npix=11.extra_file.parquet       # added after _metadata generated\n",
    "        |- Npix=11.extra_rows.parquet       # rows appended after _metadata generated\n",
    "        |- (Npix=11.missing_file.parquet)   # dropped after _metadata generated\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset_path = Path(\".\") / \"wrong_files_and_rows\" / \"dataset\"\n",
    "remove_directory(output_dataset_path.parent)\n",
    "\n",
    "# Existing files may result in unexpected metadata output.\n",
    "if output_dataset_path.parent.exists() and any(output_dataset_path.parent.iterdir()):\n",
    "    raise FileExistsError(\"wrong_files_and_rows directory exists and is not empty. Remove it and try again.\")\n",
    "\n",
    "# We will create the following files using input_frag\n",
    "ffrag_out = output_dataset_path / frag_key\n",
    "fmissing_file = ffrag_out.with_suffix(\".missing_file.parquet\")\n",
    "fextra_file = ffrag_out.with_suffix(\".extra_file.parquet\")\n",
    "fextra_rows = ffrag_out.with_suffix(\".extra_rows.parquet\")\n",
    "\n",
    "ffrag_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy metadata files that we will not alter\n",
    "shutil.copy(input_dataset_path.parent / \"properties\", output_dataset_path.parent / \"properties\")\n",
    "shutil.copy(input_dataset_path / \"_common_metadata\", output_dataset_path / \"_common_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a direct copy of input_frag for all files that will be recorded in the _metadata file\n",
    "for file_out in [ffrag_out, fmissing_file, fextra_rows]:\n",
    "    shutil.copy(input_frag.path, file_out)\n",
    "\n",
    "# Write _metadata\n",
    "collect_and_write_metadata(output_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mangle the dataset.\n",
    "\n",
    "# Add a file\n",
    "shutil.copy(input_frag.path, fextra_file)\n",
    "\n",
    "# Remove a file\n",
    "fmissing_file.unlink()\n",
    "\n",
    "# Add rows to an existing file\n",
    "new_tbl = pa.concat_tables([input_tbl, input_tbl.take([1, 2, 3, 4])])\n",
    "pq.write_table(new_tbl, fextra_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a jagged catalog (to check memory-size partitioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log of strategies tried:\n",
    "- \"Naive uniform\" radec across the sphere; arbitrary variation in row sizes\n",
    "- \"Naive uniform\" radec across the sphere; exponentially increasing row sizes\n",
    "- \"Naive uniform\" radec within a small area; exponentially increasing row sizes\n",
    "- Don't recall the radec distribution; slightly more even short to long row sizes\n",
    "- \"Naive uniform\" radec across the sphere; all positive decs have long rows, neg decs have short\n",
    "    - This has the problem of pixels at equator containing both long and short, so:\n",
    "- \"Naive uniform\" near north pole, then exactly mirrored to near-south-pole; pos decs long and neg decs short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate dec 70-90\n",
    "then flip it (mirror the data) - specifically identical, just with a flipped sign for dec\n",
    "\n",
    "note for uniform on sphere, it'd have to be sin of dec. but it's ok for now as is probably\n",
    "\n",
    "and then make the pos decs way longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated simple jagged catalog with long mag lists for positive Dec at small_sky_jagged_source/small_sky_jagged.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "jagged_path = Path(\"./small_sky_jagged_source/small_sky_jagged.parquet\")\n",
    "\n",
    "# Delete existing file if it exists\n",
    "if jagged_path.exists():\n",
    "    os.remove(jagged_path)\n",
    "\n",
    "jagged_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not jagged_path.exists():\n",
    "    num_objects = 2000  # Catalog size\n",
    "    num_objects_half = num_objects // 2\n",
    "\n",
    "    # First half: random RA, Dec in [70, 90]\n",
    "    ra_first = np.random.uniform(0, 360, num_objects_half)\n",
    "    dec_first = np.random.uniform(70, 90, num_objects_half)\n",
    "\n",
    "    # Second half: RA identical, Dec sign-flipped\n",
    "    ra = np.concatenate([ra_first, ra_first])\n",
    "    dec = np.concatenate([dec_first, -dec_first])\n",
    "\n",
    "    mag = []\n",
    "    for i in range(num_objects):\n",
    "        if i < num_objects_half:\n",
    "            # Positive Dec: long mag list (e.g., 1000 elements)\n",
    "            mag.append(np.random.uniform(15, 25, 1000).tolist())\n",
    "        else:\n",
    "            # Negative Dec: short mag list (e.g., 5 elements)\n",
    "            mag.append(np.random.uniform(15, 25, 5).tolist())\n",
    "\n",
    "    data = {\n",
    "        \"id\": np.arange(700, 700 + num_objects),\n",
    "        \"ra\": ra,\n",
    "        \"dec\": dec,\n",
    "        \"mag\": mag,\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_parquet(jagged_path, index=False)\n",
    "    print(f\"Generated simple jagged catalog with long mag lists for positive Dec at {jagged_path}\")\n",
    "else:\n",
    "    print(f\"Jagged test catalog already exists at {jagged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum bytes in a row: 8000\n",
      "Row index with max bytes: 0\n",
      "Length of 'mag' in that row: 1000\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "# Load the jagged catalog\n",
    "jagged_path = \"./small_sky_jagged_source/small_sky_jagged.parquet\"\n",
    "df = pd.read_parquet(jagged_path)\n",
    "\n",
    "# Compute the size in bytes of each row's 'mag' list (assuming float64)\n",
    "row_bytes = df[\"mag\"].apply(lambda x: np.array(x, dtype=np.float64).nbytes)\n",
    "print(f\"Maximum bytes in a row: {row_bytes.max()}\")\n",
    "print(f\"Row index with max bytes: {row_bytes.idxmax()}\")\n",
    "print(f\"Length of 'mag' in that row: {len(df.loc[row_bytes.idxmax(), 'mag'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
